{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:52:50.458563Z","iopub.execute_input":"2025-05-22T06:52:50.458832Z","iopub.status.idle":"2025-05-22T06:52:51.590756Z","shell.execute_reply.started":"2025-05-22T06:52:50.458809Z","shell.execute_reply":"2025-05-22T06:52:51.590046Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/instruction-tune-llama-2-int4.ipynb","metadata":{}},{"cell_type":"code","source":"#!pip install \"transformers==4.34.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.23.0\" \"bitsandbytes==0.41.1\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-22T08:15:22.251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install \"transformers\" \"datasets\" \"peft\" \"accelerate\" \"bitsandbytes\" \"trl\" \"safetensors\" --upgrade -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:52:51.592079Z","iopub.execute_input":"2025-05-22T06:52:51.592436Z","iopub.status.idle":"2025-05-22T06:54:16.912309Z","shell.execute_reply.started":"2025-05-22T06:52:51.592413Z","shell.execute_reply":"2025-05-22T06:54:16.911423Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# !pip install -U -q bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:42:09.306152Z","iopub.execute_input":"2025-05-22T06:42:09.306516Z","iopub.status.idle":"2025-05-22T06:42:16.066921Z","shell.execute_reply.started":"2025-05-22T06:42:09.306491Z","shell.execute_reply":"2025-05-22T06:42:16.065787Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# !pip install -U -q load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:44:11.071776Z","iopub.execute_input":"2025-05-22T06:44:11.072134Z","iopub.status.idle":"2025-05-22T06:44:13.057701Z","shell.execute_reply.started":"2025-05-22T06:44:11.072110Z","shell.execute_reply":"2025-05-22T06:44:13.056730Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement load_dataset (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for load_dataset\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from datasets import load_dataset\nfrom random import randrange\n\n# Load dataset from the hub\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n\nprint(f\"dataset size: {len(dataset)}\")\nprint(dataset[randrange(len(dataset))])\n# dataset size: 15011","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:54:54.430331Z","iopub.execute_input":"2025-05-22T06:54:54.430603Z","iopub.status.idle":"2025-05-22T06:54:59.987904Z","shell.execute_reply.started":"2025-05-22T06:54:54.430574Z","shell.execute_reply":"2025-05-22T06:54:59.987337Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05a9e5a05054167b3088c4f30192f7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2c00cb09212489c8ccb57985ef1354f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d5d5c55f9af48959dc786ab35ed143d"}},"metadata":{}},{"name":"stdout","text":"dataset size: 15011\n{'instruction': 'What state is known as the Lone Star State?', 'context': '', 'response': 'Texas.', 'category': 'open_qa'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def format_instruction(sample):\n\treturn f\"\"\"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\n{sample['response']}\n\n### Response:\n{sample['instruction']}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:55:01.935794Z","iopub.execute_input":"2025-05-22T06:55:01.936260Z","iopub.status.idle":"2025-05-22T06:55:01.940668Z","shell.execute_reply.started":"2025-05-22T06:55:01.936236Z","shell.execute_reply":"2025-05-22T06:55:01.939923Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from random import randrange\n\nprint(format_instruction(dataset[randrange(len(dataset))]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:55:05.663345Z","iopub.execute_input":"2025-05-22T06:55:05.663613Z","iopub.status.idle":"2025-05-22T06:55:05.668488Z","shell.execute_reply.started":"2025-05-22T06:55:05.663591Z","shell.execute_reply":"2025-05-22T06:55:05.667725Z"}},"outputs":[{"name":"stdout","text":"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\nMichael Jordan played basketball for the North Carolina Tar Heels, Chicago Bulls and Washington Wizards. He is the principal owner of the Charlotte Hornets of the NBA and 23XI Racing in the NASCAR Cup Series. He also played for the United States national team, winning gold medals at the 1983 Pan American Games, 1984 Summer Olympics, 1992 Tournament of the Americas and 1992 Summer Olympics. In addition, he played Minor League Baseball between stints on the Chicago Bulls between 1994 and 1995.\n\n### Response:\nExtract the names of the teams that Michael Jordan has been a part of from the text below.\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nuse_flash_attention = False\n\n# Hugging Face model id\nmodel_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_use_double_quant=True, \n    bnb_4bit_quant_type=\"nf4\", \n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    use_cache=False,\n    use_flash_attention_2=use_flash_attention,\n    device_map=\"auto\",\n)\nmodel.config.pretraining_tp = 1\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:55:08.753985Z","iopub.execute_input":"2025-05-22T06:55:08.754261Z","iopub.status.idle":"2025-05-22T06:56:26.244586Z","shell.execute_reply.started":"2025-05-22T06:55:08.754243Z","shell.execute_reply":"2025-05-22T06:56:26.243783Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b2da1f862b445acb82d56d6c9c41d40"}},"metadata":{}},{"name":"stderr","text":"2025-05-22 06:55:20.648103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747896920.828935      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747896920.881097      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f441bbbb8b4840388f4ea0b34cf95657"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97e491aa9c3b4fcabc7245dea0d1c6f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a257365d348c406d8ceec00f19f39fc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76dd8401a3b34bb4bcc3c07e529b530b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0372688ff9a4aa9b09d597bea377902"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d5db8952a564e849bb272790580eada"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deee64a575004fa18f08b182c78c2600"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6607a5baa2864d8880dc992f008e9d73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"377ce5dba5c3430d9d4d8a9906440402"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26213a0ac1f848f1836eebc217a0f135"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\n# LoRA config based on QLoRA paper\npeft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\", \n)\n\n\n# prepare model for training\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:56:26.245736Z","iopub.execute_input":"2025-05-22T06:56:26.246517Z","iopub.status.idle":"2025-05-22T06:56:26.473856Z","shell.execute_reply.started":"2025-05-22T06:56:26.246495Z","shell.execute_reply":"2025-05-22T06:56:26.473053Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"llama-7-int4-dolly\",\n    num_train_epochs=1,\n    per_device_train_batch_size=6 if use_flash_attention else 4,\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    bf16=True,\n    fp16=False,\n    tf32=False,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n    disable_tqdm=False,  # disable tqdm since with packing values are in correct\n)\n\n\n# Upcast layer for flash attnetion\nif use_flash_attention:\n    from utils.llama_patch import upcast_layer_for_flash_attention\n    torch_dtype = torch.bfloat16 if args.bf16 else torch.float16 if args.fp16 else torch.float32\n    model = upcast_layer_for_flash_attention(model, torch_dtype)\n\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:58:06.249697Z","iopub.execute_input":"2025-05-22T06:58:06.250549Z","iopub.status.idle":"2025-05-22T06:58:06.722741Z","shell.execute_reply.started":"2025-05-22T06:58:06.250524Z","shell.execute_reply":"2025-05-22T06:58:06.721930Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'NousResearch/Llama-2-7b-hf' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from trl import SFTTrainer\n\nmax_seq_length = 2048 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n   # max_seq_length=max_seq_length,\n  #  tokenizer=tokenizer,\n  #  packing=True,\n    formatting_func=format_instruction, \n    args=args,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:58:24.762509Z","iopub.execute_input":"2025-05-22T06:58:24.762797Z","iopub.status.idle":"2025-05-22T06:58:37.283502Z","shell.execute_reply.started":"2025-05-22T06:58:24.762776Z","shell.execute_reply":"2025-05-22T06:58:37.282751Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Applying formatting function to train dataset:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd901eb0e05145e38c0768774ba773e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f0952ab0bd43f7a170ae96cc51bece"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1df7805838fb498b864d5b7819ecf31f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2bc74f3a45448b6bfc478f97eb8d37d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"360a06609aee4fc9a7066f353f136aef"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# train\ntrainer.train() # there will not be a progress bar since tqdm is disabled\n\n# save model\ntrainer.save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:58:41.722517Z","iopub.execute_input":"2025-05-22T06:58:41.723300Z","execution_failed":"2025-05-22T08:15:22.251Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"if use_flash_attention:\n    # unpatch flash attention\n    from utils.llama_patch import unplace_flash_attn_with_attn\n    unplace_flash_attn_with_attn()\n    \nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n\n\nargs.output_dir = \"llama-7-int4-dolly\"\n\n# load base LLM model and tokenizer\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n) \ntokenizer = AutoTokenizer.from_pretrained(args.output_dir)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-22T08:15:22.251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset \nfrom random import randrange\n\n\n# Load dataset from the hub and get a sample\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\nsample = dataset[randrange(len(dataset))]\n\nprompt = f\"\"\"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\n{sample['response']}\n\n### Response:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n# with torch.inference_mode():\noutputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n\nprint(f\"Prompt:\\n{sample['response']}\\n\")\nprint(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\nprint(f\"Ground truth:\\n{sample['instruction']}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-22T08:15:22.251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    low_cpu_mem_usage=True,\n) \n\n# Merge LoRA and base model\nmerged_model = model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\n\n# push merged model to the hub\n# merged_model.push_to_hub(\"user/repo\")\n# tokenizer.push_to_hub(\"user/repo\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-22T08:15:22.251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}