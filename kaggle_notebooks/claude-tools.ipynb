{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-01T06:15:09.243709Z",
     "iopub.status.busy": "2025-07-01T06:15:09.243218Z",
     "iopub.status.idle": "2025-07-01T06:15:09.253617Z",
     "shell.execute_reply": "2025-07-01T06:15:09.252892Z",
     "shell.execute_reply.started": "2025-07-01T06:15:09.243670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/chinchilla/sardana24a.pdf\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:23:53.772365Z",
     "iopub.status.busy": "2025-07-01T06:23:53.771652Z",
     "iopub.status.idle": "2025-07-01T06:23:53.775548Z",
     "shell.execute_reply": "2025-07-01T06:23:53.774848Z",
     "shell.execute_reply.started": "2025-07-01T06:23:53.772344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Reference notbook - https://www.kaggle.com/code/iamrahulthorat/claude-demo/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:47:15.891018Z",
     "iopub.status.busy": "2025-07-01T05:47:15.890751Z",
     "iopub.status.idle": "2025-07-01T05:47:21.529141Z",
     "shell.execute_reply": "2025-07-01T05:47:21.528381Z",
     "shell.execute_reply.started": "2025-07-01T05:47:15.890993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install anthropic -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:45:06.085115Z",
     "iopub.status.busy": "2025-07-01T05:45:06.084366Z",
     "iopub.status.idle": "2025-07-01T05:45:06.088771Z",
     "shell.execute_reply": "2025-07-01T05:45:06.087884Z",
     "shell.execute_reply.started": "2025-07-01T05:45:06.085087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ANTHROPIC_API_KEY='key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:50:47.811075Z",
     "iopub.status.busy": "2025-07-01T05:50:47.810434Z",
     "iopub.status.idle": "2025-07-01T05:50:47.987292Z",
     "shell.execute_reply": "2025-07-01T05:50:47.986536Z",
     "shell.execute_reply.started": "2025-07-01T05:50:47.811050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "# Setting up the api key\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"claude-api-key\")\n",
    "\n",
    "client = anthropic.Anthropic(api_key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:08:35.829969Z",
     "iopub.status.busy": "2025-07-01T06:08:35.829283Z",
     "iopub.status.idle": "2025-07-01T06:08:35.834718Z",
     "shell.execute_reply": "2025-07-01T06:08:35.833665Z",
     "shell.execute_reply.started": "2025-07-01T06:08:35.829940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "opus = 'claude-3-opus-20240229'     # Most powerful model, most expensive\n",
    "sonnet = 'claude-3-sonnet-20240229' # Most balanced model between intelligence and speed\n",
    "haiku = 'claude-3-haiku-20240307'   # Fastest and most compact model\n",
    "opus4= \"claude-opus-4-20250514\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:09:36.797603Z",
     "iopub.status.busy": "2025-07-01T06:09:36.797362Z",
     "iopub.status.idle": "2025-07-01T06:09:42.186523Z",
     "shell.execute_reply": "2025-07-01T06:09:42.185736Z",
     "shell.execute_reply.started": "2025-07-01T06:09:36.797587Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ancient rivers weep and flow,\n",
      "Carrying earth's minerals below.\n",
      "Into vast basins, waters creep,\n",
      "Depositing salt they cannot keep.\n",
      "\n",
      "Sun lifts water to the sky,\n",
      "Pure vapor rises, salt stays dry.\n",
      "Eons pass, the cycle spins—\n",
      "Salt remains while water thins.\n"
     ]
    }
   ],
   "source": [
    "def model_request(model1=\"claude-opus-4-20250514\", system1=\"You are a world-class poet. Respond only with short poems.\", prompt1=\"Why is the ocean salty?\"):\n",
    "    message = client.messages.create(\n",
    "        model= model1, #\"claude-opus-4-20250514\",\n",
    "        max_tokens=1000,\n",
    "        temperature=1,\n",
    "        system= system1, #\"You are a world-class poet. Respond only with short poems.\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\":  prompt1 #\"Why is the ocean salty?\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ])\n",
    "    \n",
    "    return message.content[0].text\n",
    "\n",
    "res = model_request()\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:10:17.751795Z",
     "iopub.status.busy": "2025-07-01T06:10:17.751100Z",
     "iopub.status.idle": "2025-07-01T06:10:20.542850Z",
     "shell.execute_reply": "2025-07-01T06:10:20.542195Z",
     "shell.execute_reply.started": "2025-07-01T06:10:17.751768Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thomas.shelby@peakyblinders.com\n",
      "arthur.shelby@peakyblinders.com\n",
      "polly.gray@peakyblinders.com\n"
     ]
    }
   ],
   "source": [
    "prompt = '''Precisely extract any email addresses from the following text and then write them, one per line.\n",
    "            Only write an email address if it's precisely spelled out in the input text.\n",
    "            If there are no email addresses in the text, write \"N/A\". Do not say anything else.\n",
    "\n",
    "            Phone Directory:\n",
    "            Thomas Shelby, 555-123-4567, [thomas.shelby@peakyblinders.com]\n",
    "            Arthur Shelby, 555-987-6543, [arthur.shelby@peakyblinders.com]\n",
    "            Polly Gray, 555-345-6789, [polly.gray@peakyblinders.com]\n",
    "\n",
    "            Phone directory will be kept up to date by the HR manager.'''\n",
    "\n",
    "res = model_request(system1=\"\", prompt1=prompt)\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:10:27.362266Z",
     "iopub.status.busy": "2025-07-01T06:10:27.361959Z",
     "iopub.status.idle": "2025-07-01T06:10:28.697792Z",
     "shell.execute_reply": "2025-07-01T06:10:28.697169Z",
     "shell.execute_reply.started": "2025-07-01T06:10:27.362242Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/3019808832.py:2: DeprecationWarning: The model 'claude-3-sonnet-20240229' is deprecated and will reach end-of-life on July 21st, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  message = client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thomas.shelby@peakyblinders.com\n",
      "arthur.shelby@peakyblinders.com\n",
      "polly.gray@peakyblinders.com\n"
     ]
    }
   ],
   "source": [
    "res = model_request(model1 = sonnet , system1=\"\" ,prompt1=prompt)\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:16:32.500374Z",
     "iopub.status.busy": "2025-07-01T06:16:32.500065Z",
     "iopub.status.idle": "2025-07-01T06:16:36.048394Z",
     "shell.execute_reply": "2025-07-01T06:16:36.047622Z",
     "shell.execute_reply.started": "2025-07-01T06:16:32.500344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:16:52.394310Z",
     "iopub.status.busy": "2025-07-01T06:16:52.393726Z",
     "iopub.status.idle": "2025-07-01T06:16:52.875605Z",
     "shell.execute_reply": "2025-07-01T06:16:52.875058Z",
     "shell.execute_reply.started": "2025-07-01T06:16:52.394283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:20:06.764174Z",
     "iopub.status.busy": "2025-07-01T06:20:06.763591Z",
     "iopub.status.idle": "2025-07-01T06:20:07.415620Z",
     "shell.execute_reply": "2025-07-01T06:20:07.414848Z",
     "shell.execute_reply.started": "2025-07-01T06:20:06.764150Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyond Chinchilla-Optimal:\n",
      "Accounting for Inference in Language Model Scaling Laws\n",
      "Nikhil Sardana1Jacob Portes1Sasha Doubov1Jonathan Frankle1\n",
      "Abstract\n",
      "Large language model (LLM) scaling laws are em-\n",
      "pirical formulas that estimate changes in model\n",
      "quality as a result of increasing parameter count\n",
      "and training data. However, these formulas, in-\n",
      "cluding the popular Deepmind Chinchilla scaling\n",
      "laws, neglect to include the cost of inference. We\n",
      "modify the Chinchilla scaling laws to calculate the\n",
      "optimal LLM parameter count and pre-training\n",
      "data size to train and deploy a model of a given\n",
      "quality and inference demand. We conduct our\n",
      "analysis both in terms of a compute budget and\n",
      "real-world costs and find that LLM researchers ex-\n",
      "pecting reasonably large inference demand ( ⁄tildelow1B\n",
      "requests) should train models smaller and longer\n",
      "than Chinchilla-optimal. Furthermore, we train\n",
      "47 models of varying sizes and parameter counts\n",
      "to validate our formula and find that model quality\n",
      "continues to im\n"
     ]
    }
   ],
   "source": [
    "with open(\"/kaggle/input/chinchilla/sardana24a.pdf\", \"rb\") as file:\n",
    "    reader = PdfReader(file)\n",
    "    num_of_pages = len(reader.pages)\n",
    "    case_of_identity = \"\".join(page.extract_text() for page in reader.pages)\n",
    "    print (case_of_identity[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:20:50.807499Z",
     "iopub.status.busy": "2025-07-01T06:20:50.807214Z",
     "iopub.status.idle": "2025-07-01T06:20:59.160944Z",
     "shell.execute_reply": "2025-07-01T06:20:59.160238Z",
     "shell.execute_reply.started": "2025-07-01T06:20:50.807477Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/3019808832.py:2: DeprecationWarning: The model 'claude-3-sonnet-20240229' is deprecated and will reach end-of-life on July 21st, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  message = client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two sentences summarizing the key points about accounting for inference in language model scaling laws:\n",
      "\n",
      "1) The paper modifies the Chinchilla scaling laws to account for the computational costs of inference, showing that for high inference demand, it is optimal to train smaller models on more data than prescribed by the original Chinchilla scaling laws. \n",
      "\n",
      "2) Experiments training models at extreme token/parameter ratios up to 10,000 show that model quality as measured by loss and downstream metrics continues improving with more data, contradicting the hypothesis that there is a \"critical size\" below which models cannot match larger Chinchilla-style models.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Summarize the following case of identity story in 2 sentences: ' + case_of_identity\n",
    "\n",
    "# Call the function with the inputs\n",
    "res = model_request(model1=sonnet, system1= \"\",  prompt1=prompt)\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRompt Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:25:02.735398Z",
     "iopub.status.busy": "2025-07-01T06:25:02.734772Z",
     "iopub.status.idle": "2025-07-01T06:25:32.973572Z",
     "shell.execute_reply": "2025-07-01T06:25:32.972801Z",
     "shell.execute_reply.started": "2025-07-01T06:25:02.735375Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"input_tokens\":65,\"output_tokens\":531,\"server_tool_use\":null,\"service_tier\":\"standard\"}\n",
      "{\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"input_tokens\":51,\"output_tokens\":463,\"server_tool_use\":null,\"service_tier\":\"standard\"}\n"
     ]
    }
   ],
   "source": [
    "# import anthropic\n",
    "\n",
    "# client = anthropic.Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-opus-4-20250514\",\n",
    "    max_tokens=1024,\n",
    "    system=[\n",
    "      {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\",\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"<the entire contents of 'Pride and Prejudice'>\",\n",
    "        \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "      }\n",
    "    ],\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Analyze the major themes in 'Pride and Prejudice'.\"}],\n",
    ")\n",
    "print(response.usage.model_dump_json())\n",
    "\n",
    "# Call the model again with the same inputs up to the cache checkpoint\n",
    "response = client.messages.create(\n",
    "     model=\"claude-opus-4-20250514\",\n",
    "    max_tokens=1024,\n",
    "    system=[\n",
    "      {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\",\n",
    "      },\n",
    "     \n",
    "    ],\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Analyze the major themes in 'Pride and Prejudice'.\"}],\n",
    "\n",
    ")\n",
    "print(response.usage.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:26:31.323834Z",
     "iopub.status.busy": "2025-07-01T06:26:31.323115Z",
     "iopub.status.idle": "2025-07-01T06:26:58.778545Z",
     "shell.execute_reply": "2025-07-01T06:26:58.777748Z",
     "shell.execute_reply.started": "2025-07-01T06:26:31.323809Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking summary: This is asking about primes that are congruent to 3 modulo 4, i.e., primes of the form 4k + 3.\n",
      "\n",
      "Let me think about this systematically. I need to prove that there are infinitely many primes p such that p ≡ 3 (mod 4).\n",
      "\n",
      "This is a classic result in number theory. Let me try to prove it using a proof by contradiction, similar to Euclid's proof for the infinitude of primes, but with a twist.\n",
      "\n",
      "Assume for contradiction that there are only finitely many primes congruent to 3 modulo 4. Let's call them p₁, p₂, ..., pₖ.\n",
      "\n",
      "Now I need to construct a number that must have a prime factor congruent to 3 modulo 4, but which is not divisible by any of p₁, p₂, ..., pₖ.\n",
      "\n",
      "Consider the number N = 4(p₁p₂...pₖ) - 1.\n",
      "\n",
      "Note that N ≡ -1 ≡ 3 (mod 4).\n",
      "\n",
      "Now, N is not divisible by any of p₁, p₂, ..., pₖ because:\n",
      "- If pᵢ divides N, then pᵢ divides 4(p₁p₂...pₖ) - 1\n",
      "- This means 4(p₁p₂...pₖ) ≡ 1 (mod pᵢ)\n",
      "- Since pᵢ divides p₁p₂...pₖ, we have p₁p₂...pₖ ≡ 0 (mod pᵢ)\n",
      "- So 4 · 0 ≡ 1 (mod pᵢ), which means 0 ≡ 1 (mod pᵢ)\n",
      "\n",
      "\n",
      "- This is impossible for any prime pᵢ > 1\n",
      "\n",
      "I recognize this contradiction means N must have a prime factor not in our original list. Moreover, this new prime factor must be congruent to 3 modulo 4. The key insight is that any number congruent to 3 modulo 4 cannot be formed by multiplying numbers all congruent to 1 modulo 4.\n",
      "\n",
      "When examining primes modulo 4, we see three possibilities: 2, numbers congruent to 1, and numbers congruent to 3. The constraint is clear: a product of numbers all congruent to 1 modulo 4 will always remain congruent to 1 modulo 4.\n",
      "\n",
      "This means N's prime factorization must include at least one prime factor that breaks this pattern, specifically a prime congruent to 3 modulo 4.\n",
      "\n",
      "However, we've already established that N is not divisible by the known primes congruent to 3 modulo 4. This creates an irreconcilable logical contradiction, suggesting our initial assumption must be fundamentally flawed.\n",
      "\n",
      "The implication points toward a profound mathematical conclusion about the distribution of primes, hinting at an infinite set of primes with specific modular characteristics.\n",
      "\n",
      "Response: Yes! There are infinitely many prime numbers such that n ≡ 3 (mod 4).\n",
      "\n",
      "Here's a proof by contradiction:\n",
      "\n",
      "**Proof:**\n",
      "\n",
      "Assume, for the sake of contradiction, that there are only finitely many primes congruent to 3 modulo 4. Let's call them p₁, p₂, ..., pₖ.\n",
      "\n",
      "Consider the number:\n",
      "N = 4(p₁p₂...pₖ) - 1\n",
      "\n",
      "Notice that:\n",
      "- N ≡ -1 ≡ 3 (mod 4)\n",
      "- N is not divisible by any of p₁, p₂, ..., pₖ\n",
      "\n",
      "The second point holds because if some pᵢ divided N, then:\n",
      "4(p₁p₂...pₖ) ≡ 1 (mod pᵢ)\n",
      "\n",
      "But since pᵢ divides the product p₁p₂...pₖ, we'd have:\n",
      "4 · 0 ≡ 1 (mod pᵢ)\n",
      "\n",
      "This gives us 0 ≡ 1 (mod pᵢ), which is impossible.\n",
      "\n",
      "Now, N must have some prime factorization. The key insight is that **if a number is congruent to 3 modulo 4, it cannot be expressed as a product consisting entirely of numbers congruent to 1 modulo 4**.\n",
      "\n",
      "This is because 1 · 1 ≡ 1 (mod 4), so any product of numbers all congruent to 1 mod 4 will itself be congruent to 1 mod 4.\n",
      "\n",
      "Since N ≡ 3 (mod 4), and the only possibilities for primes modulo 4 are:\n",
      "- p = 2 (but N is odd, so 2 doesn't divide N)\n",
      "- p ≡ 1 (mod 4)  \n",
      "- p ≡ 3 (mod 4)\n",
      "\n",
      "N must have at least one prime factor congruent to 3 modulo 4.\n",
      "\n",
      "But this contradicts our assumption that p₁, p₂, ..., pₖ were all such primes, since N isn't divisible by any of them.\n",
      "\n",
      "Therefore, there must be infinitely many primes congruent to 3 modulo 4. ∎\n",
      "\n",
      "Examples of such primes include: 3, 7, 11, 19, 23, 31, 43, 47, ...\n"
     ]
    }
   ],
   "source": [
    "# import anthropic\n",
    "\n",
    "# client = anthropic.Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=16000,\n",
    "    thinking={\n",
    "        \"type\": \"enabled\",\n",
    "        \"budget_tokens\": 10000\n",
    "    },\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "# The response will contain summarized thinking blocks and text blocks\n",
    "for block in response.content:\n",
    "    if block.type == \"thinking\":\n",
    "        print(f\"\\nThinking summary: {block.thinking}\")\n",
    "    elif block.type == \"text\":\n",
    "        print(f\"\\nResponse: {block.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:28:15.121483Z",
     "iopub.status.busy": "2025-07-01T06:28:15.120906Z",
     "iopub.status.idle": "2025-07-01T06:28:18.926942Z",
     "shell.execute_reply": "2025-07-01T06:28:18.926293Z",
     "shell.execute_reply.started": "2025-07-01T06:28:15.121462Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_01M19euXE7ukGQ5uCn5ASief', content=[TextBlock(citations=None, text='Based on the document provided:\\n\\n', type='text'), TextBlock(citations=[CitationCharLocation(cited_text='The grass is green. ', document_index=0, document_title='My Document', end_char_index=20, start_char_index=0, type='char_location')], text='The grass is green.', type='text'), TextBlock(citations=None, text=' ', type='text'), TextBlock(citations=[CitationCharLocation(cited_text='The sky is blue.', document_index=0, document_title='My Document', end_char_index=36, start_char_index=20, type='char_location')], text='The sky is blue.', type='text')], model='claude-opus-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=610, output_tokens=52, server_tool_use=None, service_tier='standard'))\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-opus-4-20250514\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"document\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"media_type\": \"text/plain\",\n",
    "                        \"data\": \"The grass is green. The sky is blue.\"\n",
    "                    },\n",
    "                    \"title\": \"My Document\",\n",
    "                    \"context\": \"This is a trustworthy document.\",\n",
    "                    \"citations\": {\"enabled\": True}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What color is the grass and sky?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:29:36.486559Z",
     "iopub.status.busy": "2025-07-01T06:29:36.485884Z",
     "iopub.status.idle": "2025-07-01T06:29:36.492539Z",
     "shell.execute_reply": "2025-07-01T06:29:36.491939Z",
     "shell.execute_reply.started": "2025-07-01T06:29:36.486533Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextBlock(citations=None, text='Based on the document provided:\\n\\n', type='text'),\n",
       " TextBlock(citations=[CitationCharLocation(cited_text='The grass is green. ', document_index=0, document_title='My Document', end_char_index=20, start_char_index=0, type='char_location')], text='The grass is green.', type='text'),\n",
       " TextBlock(citations=None, text=' ', type='text'),\n",
       " TextBlock(citations=[CitationCharLocation(cited_text='The sky is blue.', document_index=0, document_title='My Document', end_char_index=36, start_char_index=20, type='char_location')], text='The sky is blue.', type='text')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T06:30:51.291173Z",
     "iopub.status.busy": "2025-07-01T06:30:51.290831Z",
     "iopub.status.idle": "2025-07-01T06:30:57.136679Z",
     "shell.execute_reply": "2025-07-01T06:30:57.135954Z",
     "shell.execute_reply.started": "2025-07-01T06:30:51.291129Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_019tRK49v4QyRJbTTADdswG6', content=[TextBlock(citations=None, text='I\\'ve reviewed the document you provided, but I cannot find any information about API features in it. The document appears to contain only placeholder text (\"This is a very long document with thousands of words...\" followed by many ellipses), rather than actual content about APIs or their features.\\n\\nIf you have a different document that contains information about API features, please share that instead, and I\\'ll be happy to help you analyze it.', type='text')], model='claude-opus-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=2548, cache_read_input_tokens=0, input_tokens=13, output_tokens=90, server_tool_use=None, service_tier='standard'))\n"
     ]
    }
   ],
   "source": [
    "# Long document content (e.g., technical documentation)\n",
    "long_document = \"This is a very long document with thousands of words...\" + \" ... \" * 1000  # Minimum cacheable length\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-opus-4-20250514\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"document\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"media_type\": \"text/plain\",\n",
    "                        \"data\": long_document\n",
    "                    },\n",
    "                    \"citations\": {\"enabled\": True},\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"}  # Cache the document content\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What does this document say about API features?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-01T07:03:46.806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initial request with web search\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Search for comprehensive information about quantum computing breakthroughs in 2025\"\n",
    "        }\n",
    "    ],\n",
    "    tools=[{\n",
    "        \"type\": \"web_search_20250305\",\n",
    "        \"name\": \"web_search\",\n",
    "        \"max_uses\": 10\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Check if the response has pause_turn stop reason\n",
    "if response.stop_reason == \"pause_turn\":\n",
    "    # Continue the conversation with the paused content\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Search for comprehensive information about quantum computing breakthroughs in 2025\"},\n",
    "        {\"role\": \"assistant\", \"content\": response.content}\n",
    "    ]\n",
    "    \n",
    "    # Send the continuation request\n",
    "    continuation = client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-latest\",\n",
    "        max_tokens=1024,\n",
    "        messages=messages,\n",
    "        tools=[{\n",
    "            \"type\": \"web_search_20250305\",\n",
    "            \"name\": \"web_search\",\n",
    "            \"max_uses\": 10\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    print(continuation)\n",
    "else:\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7290734,
     "sourceId": 11621835,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
