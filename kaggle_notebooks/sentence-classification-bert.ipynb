{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12925059,"sourceType":"datasetVersion","datasetId":8178678}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:48:55.573045Z","iopub.status.idle":"2025-08-31T21:48:55.573327Z","shell.execute_reply.started":"2025-08-31T21:48:55.573189Z","shell.execute_reply":"2025-08-31T21:48:55.573201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print (\"hello\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:48:55.574736Z","iopub.status.idle":"2025-08-31T21:48:55.574979Z","shell.execute_reply.started":"2025-08-31T21:48:55.574870Z","shell.execute_reply":"2025-08-31T21:48:55.574884Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Reference: https://github.com/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb\n###  https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/spamdata/spamdata_v2.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:20:09.539555Z","iopub.execute_input":"2025-08-31T20:20:09.540005Z","iopub.status.idle":"2025-08-31T20:20:09.600683Z","shell.execute_reply.started":"2025-08-31T20:20:09.539982Z","shell.execute_reply":"2025-08-31T20:20:09.599917Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   label                                               text\n0      0  Go until jurong point, crazy.. Available only ...\n1      0                      Ok lar... Joking wif u oni...\n2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n3      0  U dun say so early hor... U c already then say...\n4      0  Nah I don't think he goes to usf, he lives aro...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:20:09.907132Z","iopub.execute_input":"2025-08-31T20:20:09.907372Z","iopub.status.idle":"2025-08-31T20:20:09.912060Z","shell.execute_reply.started":"2025-08-31T20:20:09.907352Z","shell.execute_reply":"2025-08-31T20:20:09.911335Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(5572, 2)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"!pip install -q transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:20:12.012717Z","iopub.execute_input":"2025-08-31T20:20:12.013253Z","iopub.status.idle":"2025-08-31T20:20:17.804363Z","shell.execute_reply.started":"2025-08-31T20:20:12.013226Z","shell.execute_reply":"2025-08-31T20:20:17.803532Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# specify GPU\ndevice = torch.device(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:20:17.805916Z","iopub.execute_input":"2025-08-31T20:20:17.806188Z","iopub.status.idle":"2025-08-31T20:20:30.979422Z","shell.execute_reply.started":"2025-08-31T20:20:17.806150Z","shell.execute_reply":"2025-08-31T20:20:30.978823Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# check class distribution\ndf['label'].value_counts(normalize = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:20:30.980097Z","iopub.execute_input":"2025-08-31T20:20:30.980504Z","iopub.status.idle":"2025-08-31T20:20:30.994889Z","shell.execute_reply.started":"2025-08-31T20:20:30.980484Z","shell.execute_reply":"2025-08-31T20:20:30.994198Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"label\n0    0.865937\n1    0.134063\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n                                                                    random_state=2018, \n                                                                    test_size=0.3, \n                                                                    stratify=df['label'])\n\n# we will use temp_text and temp_labels to create validation and test set\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=2018, \n                                                                test_size=0.5, \n                                                                stratify=temp_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:20:30.996525Z","iopub.execute_input":"2025-08-31T20:20:30.996879Z","iopub.status.idle":"2025-08-31T20:20:31.036565Z","shell.execute_reply.started":"2025-08-31T20:20:30.996852Z","shell.execute_reply":"2025-08-31T20:20:31.036079Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# import BERT-base pretrained model\nbert = AutoModel.from_pretrained('bert-base-uncased')\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:20:31.037184Z","iopub.execute_input":"2025-08-31T20:20:31.037366Z","iopub.status.idle":"2025-08-31T20:21:04.550464Z","shell.execute_reply.started":"2025-08-31T20:20:31.037352Z","shell.execute_reply":"2025-08-31T20:21:04.549664Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df3c544c81c141439f18475095962d3c"}},"metadata":{}},{"name":"stderr","text":"2025-08-31 20:20:45.025755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756671645.385476      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756671645.488594      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5286bf0fc3e2405f9c313e340efbb5b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a408a55fc8f94b6b96ed7e4bf03bcea5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61b1213a9d5046c4856ce67aa46b2e14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"839ab449a5014701ac466b9b90609d56"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# sample data\ntext = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n\n# encode text\nsent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:21:04.551331Z","iopub.execute_input":"2025-08-31T20:21:04.552174Z","iopub.status.idle":"2025-08-31T20:21:04.565564Z","shell.execute_reply.started":"2025-08-31T20:21:04.552152Z","shell.execute_reply":"2025-08-31T20:21:04.564747Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# output\nprint(sent_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:21:04.566465Z","iopub.execute_input":"2025-08-31T20:21:04.566652Z","iopub.status.idle":"2025-08-31T20:21:04.578039Z","shell.execute_reply.started":"2025-08-31T20:21:04.566637Z","shell.execute_reply":"2025-08-31T20:21:04.577284Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# get length of all the messages in the train set\nseq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:21:04.578842Z","iopub.execute_input":"2025-08-31T20:21:04.579101Z","iopub.status.idle":"2025-08-31T20:21:04.998911Z","shell.execute_reply.started":"2025-08-31T20:21:04.579079Z","shell.execute_reply":"2025-08-31T20:21:04.998152Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"max_seq_len = 25","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:21:04.999780Z","iopub.execute_input":"2025-08-31T20:21:05.000133Z","iopub.status.idle":"2025-08-31T20:21:05.003819Z","shell.execute_reply.started":"2025-08-31T20:21:05.000108Z","shell.execute_reply":"2025-08-31T20:21:05.003048Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = max_seq_len,\n    # pad_to_max_length=True,\n     padding='max_length',\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = max_seq_len,\n    # pad_to_max_length=True,\n     padding='max_length',\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = max_seq_len,\n    # pad_to_max_length=True,\n     padding='max_length',\n    truncation=True,\n    return_token_type_ids=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:21:05.006157Z","iopub.execute_input":"2025-08-31T20:21:05.006446Z","iopub.status.idle":"2025-08-31T20:21:05.314134Z","shell.execute_reply.started":"2025-08-31T20:21:05.006422Z","shell.execute_reply":"2025-08-31T20:21:05.313381Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Convert integer seq to tokens","metadata":{}},{"cell_type":"code","source":"# for train set\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\n# for validation set\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\n# for test set\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:21:05.315097Z","iopub.execute_input":"2025-08-31T20:21:05.315375Z","iopub.status.idle":"2025-08-31T20:21:05.369900Z","shell.execute_reply.started":"2025-08-31T20:21:05.315351Z","shell.execute_reply":"2025-08-31T20:21:05.368921Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## create data loaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:21:05.370824Z","iopub.execute_input":"2025-08-31T20:21:05.371164Z","iopub.status.idle":"2025-08-31T20:21:05.389193Z","shell.execute_reply.started":"2025-08-31T20:21:05.371141Z","shell.execute_reply":"2025-08-31T20:21:05.388515Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Freeze BERT parameters","metadata":{}},{"cell_type":"code","source":"# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:21:05.390109Z","iopub.execute_input":"2025-08-31T20:21:05.390380Z","iopub.status.idle":"2025-08-31T20:21:05.409294Z","shell.execute_reply.started":"2025-08-31T20:21:05.390358Z","shell.execute_reply":"2025-08-31T20:21:05.408589Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Define Model architecture","metadata":{}},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n      \n      super(BERT_Arch, self).__init__()\n\n      self.bert = bert \n      \n      # dropout layer\n      self.dropout = nn.Dropout(0.1)\n      \n      # relu activation function\n      self.relu =  nn.ReLU()\n\n      # dense layer 1\n      self.fc1 = nn.Linear(768,512)\n      \n      # dense layer 2 (Output layer)\n      self.fc2 = nn.Linear(512,2)\n\n      #softmax activation function\n      self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n\n      #pass the inputs to the model  \n      # _, cls_hs = self.bert(sent_id, attention_mask=mask)\n      outputs = self.bert(sent_id, attention_mask=mask)\n      cls_hs = outputs.pooler_output\n      \n      x = self.fc1(cls_hs)\n\n      x = self.relu(x)\n\n      x = self.dropout(x)\n\n      # output layer\n      x = self.fc2(x)\n      \n      # apply softmax activation\n      x = self.softmax(x)\n\n      return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:27:24.374058Z","iopub.execute_input":"2025-08-31T20:27:24.374832Z","iopub.status.idle":"2025-08-31T20:27:24.380512Z","shell.execute_reply.started":"2025-08-31T20:27:24.374806Z","shell.execute_reply":"2025-08-31T20:27:24.379754Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:27:25.918367Z","iopub.execute_input":"2025-08-31T20:27:25.918652Z","iopub.status.idle":"2025-08-31T20:27:25.928753Z","shell.execute_reply.started":"2025-08-31T20:27:25.918631Z","shell.execute_reply":"2025-08-31T20:27:25.927941Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# optimizer from hugging face transformers\nimport torch.optim as optim\n\n# define the optimizer\noptimizer = optim.AdamW(model.parameters(), lr = 1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:27:26.116573Z","iopub.execute_input":"2025-08-31T20:27:26.116805Z","iopub.status.idle":"2025-08-31T20:27:26.121323Z","shell.execute_reply.started":"2025-08-31T20:27:26.116788Z","shell.execute_reply":"2025-08-31T20:27:26.120480Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"### Find class weights","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n\n#compute the class weights\n# class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n#compute the class weights\nclass_wts = compute_class_weight(class_weight='balanced', classes= np.unique(train_labels), y=train_labels)\n\nprint(class_wts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:27:28.234304Z","iopub.execute_input":"2025-08-31T20:27:28.234840Z","iopub.status.idle":"2025-08-31T20:27:28.241218Z","shell.execute_reply.started":"2025-08-31T20:27:28.234819Z","shell.execute_reply":"2025-08-31T20:27:28.240333Z"}},"outputs":[{"name":"stdout","text":"[0.57743559 3.72848948]\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# convert class weights to tensor\nweights= torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:27:28.808422Z","iopub.execute_input":"2025-08-31T20:27:28.808875Z","iopub.status.idle":"2025-08-31T20:27:28.813137Z","shell.execute_reply.started":"2025-08-31T20:27:28.808854Z","shell.execute_reply":"2025-08-31T20:27:28.812584Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"### Fine Tune BERT","metadata":{}},{"cell_type":"code","source":"# function to train the model\ndef train():\n  \n  model.train()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save model predictions\n  total_preds=[]\n  \n  # iterate over batches\n  for step,batch in enumerate(train_dataloader):\n    \n    # progress update after every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n    # push the batch to gpu\n    batch = [r.to(device) for r in batch]\n \n    sent_id, mask, labels = batch\n\n    # clear previously calculated gradients \n    model.zero_grad()        \n\n    # get model predictions for the current batch\n    preds = model(sent_id, mask)\n\n    # compute the loss between actual and predicted values\n    loss = cross_entropy(preds, labels)\n\n    # add on to the total loss\n    total_loss = total_loss + loss.item()\n\n    # backward pass to calculate the gradients\n    loss.backward()\n\n    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n    # update parameters\n    optimizer.step()\n\n    # model predictions are stored on GPU. So, push it to CPU\n    preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n  # compute the training loss of the epoch\n  avg_loss = total_loss / len(train_dataloader)\n  \n  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  #returns the loss and predictions\n  return avg_loss, total_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:27:30.473094Z","iopub.execute_input":"2025-08-31T20:27:30.473443Z","iopub.status.idle":"2025-08-31T20:27:30.480276Z","shell.execute_reply.started":"2025-08-31T20:27:30.473411Z","shell.execute_reply":"2025-08-31T20:27:30.479575Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n  \n  print(\"\\nEvaluating...\")\n  \n  # deactivate dropout layers\n  model.eval()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save the model predictions\n  total_preds = []\n\n  # iterate over batches\n  for step,batch in enumerate(val_dataloader):\n    \n    # Progress update every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      \n      # Calculate elapsed time in minutes.\n      elapsed = format_time(time.time() - t0)\n            \n      # Report progress.\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n    # push the batch to gpu\n    batch = [t.to(device) for t in batch]\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n      \n      # model predictions\n      preds = model(sent_id, mask)\n\n      # compute the validation loss between actual and predicted values\n      loss = cross_entropy(preds,labels)\n\n      total_loss = total_loss + loss.item()\n\n      preds = preds.detach().cpu().numpy()\n\n      total_preds.append(preds)\n\n  # compute the validation loss of the epoch\n  avg_loss = total_loss / len(val_dataloader) \n\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  return avg_loss, total_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:27:33.139627Z","iopub.execute_input":"2025-08-31T20:27:33.140474Z","iopub.status.idle":"2025-08-31T20:27:33.146432Z","shell.execute_reply.started":"2025-08-31T20:27:33.140438Z","shell.execute_reply":"2025-08-31T20:27:33.145899Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"### Start model training","metadata":{}},{"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:27:36.303572Z","iopub.execute_input":"2025-08-31T20:27:36.304557Z","iopub.status.idle":"2025-08-31T20:29:02.398365Z","shell.execute_reply.started":"2025-08-31T20:27:36.304534Z","shell.execute_reply":"2025-08-31T20:29:02.397758Z"}},"outputs":[{"name":"stdout","text":"\n Epoch 1 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.523\nValidation Loss: 0.308\n\n Epoch 2 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.319\nValidation Loss: 0.244\n\n Epoch 3 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.262\nValidation Loss: 0.260\n\n Epoch 4 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.253\nValidation Loss: 0.346\n\n Epoch 5 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.267\nValidation Loss: 0.156\n\n Epoch 6 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.244\nValidation Loss: 0.765\n\n Epoch 7 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.238\nValidation Loss: 0.179\n\n Epoch 8 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.223\nValidation Loss: 0.158\n\n Epoch 9 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.207\nValidation Loss: 0.226\n\n Epoch 10 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.183\nValidation Loss: 0.214\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## load saved model","metadata":{}},{"cell_type":"code","source":"#load weights of best model\npath = 'saved_weights.pt'\nmodel.load_state_dict(torch.load(path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:29:02.399740Z","iopub.execute_input":"2025-08-31T20:29:02.399981Z","iopub.status.idle":"2025-08-31T20:29:02.797179Z","shell.execute_reply.started":"2025-08-31T20:29:02.399962Z","shell.execute_reply":"2025-08-31T20:29:02.796457Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":48},{"cell_type":"markdown","source":"### Get predictions","metadata":{}},{"cell_type":"code","source":"# get predictions for test data\nwith torch.no_grad():\n  preds = model(test_seq.to(device), test_mask.to(device))\n  preds = preds.detach().cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:29:02.798081Z","iopub.execute_input":"2025-08-31T20:29:02.798329Z","iopub.status.idle":"2025-08-31T20:29:03.981170Z","shell.execute_reply.started":"2025-08-31T20:29:02.798313Z","shell.execute_reply":"2025-08-31T20:29:03.980561Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# model's performance\npreds = np.argmax(preds, axis = 1)\nprint(classification_report(test_y, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:29:03.982555Z","iopub.execute_input":"2025-08-31T20:29:03.983154Z","iopub.status.idle":"2025-08-31T20:29:03.994575Z","shell.execute_reply.started":"2025-08-31T20:29:03.983134Z","shell.execute_reply":"2025-08-31T20:29:03.994053Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       724\n           1       0.73      0.96      0.83       112\n\n    accuracy                           0.95       836\n   macro avg       0.86      0.96      0.90       836\nweighted avg       0.96      0.95      0.95       836\n\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# confusion matrix\npd.crosstab(test_y, preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T20:29:16.573443Z","iopub.execute_input":"2025-08-31T20:29:16.573752Z","iopub.status.idle":"2025-08-31T20:29:16.637007Z","shell.execute_reply.started":"2025-08-31T20:29:16.573729Z","shell.execute_reply":"2025-08-31T20:29:16.636443Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"col_0    0    1\nrow_0          \n0      685   39\n1        4  108","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>col_0</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>row_0</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>685</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>108</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":51},{"cell_type":"markdown","source":"### Using Transformer Library","metadata":{}},{"cell_type":"markdown","source":"## Reference: https://towardsdatascience.com/fine-tune-smaller-transformer-models-text-classification-77cbbd3bf02b/","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n\ndataset = load_dataset(\"ilsilfverskiold/clickbait_titles_synthetic_data\")\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:52:30.298335Z","iopub.execute_input":"2025-08-31T21:52:30.298528Z","iopub.status.idle":"2025-08-31T21:52:36.620847Z","shell.execute_reply.started":"2025-08-31T21:52:30.298509Z","shell.execute_reply":"2025-08-31T21:52:36.620245Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec33d7c8e12048008bbd9ba21a2237ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/337k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7849c130283c4011a820ceecb1cf3d54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/20.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd9b59ff0d8d4d7397c8a9751fc5b53f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/42.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70fa2e22a03048b0a2ae5b249f3abb1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/6614 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6636418c68b7468c9103993ddb3b7bcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63020c352c634638b668e103fd4e0c24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f500199f6a54419864e2a9f3db92064"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'text', '__index_level_0__'],\n        num_rows: 6614\n    })\n    validation: Dataset({\n        features: ['label', 'text', '__index_level_0__'],\n        num_rows: 350\n    })\n    test: Dataset({\n        features: ['label', 'text', '__index_level_0__'],\n        num_rows: 818\n    })\n})"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"model_name = \"albert/albert-base-v2\"\nyour_path = \"classify-clickbait\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:52:44.517234Z","iopub.execute_input":"2025-08-31T21:52:44.517762Z","iopub.status.idle":"2025-08-31T21:52:44.521212Z","shell.execute_reply.started":"2025-08-31T21:52:44.517737Z","shell.execute_reply":"2025-08-31T21:52:44.520441Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\nlabel_encoder.fit(dataset['train']['label'])\n\ndef encode_labels(example):\n    return {'encoded_label': label_encoder.transform([example['label']])[0]}\n\nfor split in dataset:\n    dataset[split] = dataset[split].map(encode_labels, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:52:46.506659Z","iopub.execute_input":"2025-08-31T21:52:46.507440Z","iopub.status.idle":"2025-08-31T21:52:47.767089Z","shell.execute_reply.started":"2025-08-31T21:52:46.507413Z","shell.execute_reply":"2025-08-31T21:52:47.766255Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6614 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bdb7cff0bae46babe349284d32efed3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd251dac377c45c3a4c31f32c3bff3d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74b920fcd76c4db49009e7fd5423153c"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoConfig\n\nunique_labels = sorted(list(set(dataset['train']['label'])))\nid2label = {i: label for i, label in enumerate(unique_labels)}\nlabel2id = {label: i for i, label in enumerate(unique_labels)}\n\nconfig = AutoConfig.from_pretrained(model_name)\nconfig.id2label = id2label\nconfig.label2id = label2id\n\n# Verify the correct labels\nprint(\"ID to Label Mapping:\", config.id2label)\nprint(\"Label to ID Mapping:\", config.label2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:52:51.650209Z","iopub.execute_input":"2025-08-31T21:52:51.650876Z","iopub.status.idle":"2025-08-31T21:52:58.430803Z","shell.execute_reply.started":"2025-08-31T21:52:51.650841Z","shell.execute_reply":"2025-08-31T21:52:58.430075Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"043bfe4c83a940329cafb0e482b0dd5d"}},"metadata":{}},{"name":"stdout","text":"ID to Label Mapping: {0: 'Clickbait', 1: 'Factual'}\nLabel to ID Mapping: {'Clickbait': 0, 'Factual': 1}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\n# specify GPU\ndevice = torch.device(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:53:14.334712Z","iopub.execute_input":"2025-08-31T21:53:14.335443Z","iopub.status.idle":"2025-08-31T21:53:14.339632Z","shell.execute_reply.started":"2025-08-31T21:53:14.335405Z","shell.execute_reply":"2025-08-31T21:53:14.338719Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import AlbertForSequenceClassification, AlbertTokenizer\n\ntokenizer = AlbertTokenizer.from_pretrained(model_name)\nmodel = AlbertForSequenceClassification.from_pretrained(model_name, config=config)\n# specify GPU\n# model = model.to_device(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:53:25.423121Z","iopub.execute_input":"2025-08-31T21:53:25.423396Z","iopub.status.idle":"2025-08-31T21:53:47.139417Z","shell.execute_reply.started":"2025-08-31T21:53:25.423374Z","shell.execute_reply":"2025-08-31T21:53:47.138360Z"}},"outputs":[{"name":"stderr","text":"2025-08-31 21:53:32.759749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756677213.000427      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756677213.067083      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"018e6aa76ee24cb8be4e779f3ad8859f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"472df3212b3b40ed8a7e534ad5eed16d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"262ba3994b124a088404720af9f1af2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3940641bebba4ea58967d7791ed52eb6"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2349477974.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# specify GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n","\u001b[0;31mAttributeError\u001b[0m: 'AlbertForSequenceClassification' object has no attribute 'to_device'"],"ename":"AttributeError","evalue":"'AlbertForSequenceClassification' object has no attribute 'to_device'","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"def filter_invalid_content(example):\n    return isinstance(example['text'], str)\n\ndataset = dataset.filter(filter_invalid_content, batched=False)\n\ndef encode_data(batch):\n    tokenized_inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=256)\n    tokenized_inputs[\"labels\"] = batch[\"encoded_label\"]\n    return tokenized_inputs\n\ndataset_encoded = dataset.map(encode_data, batched=True)\ndataset_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:54:09.531818Z","iopub.execute_input":"2025-08-31T21:54:09.532339Z","iopub.status.idle":"2025-08-31T21:54:11.386646Z","shell.execute_reply.started":"2025-08-31T21:54:09.532292Z","shell.execute_reply":"2025-08-31T21:54:11.385895Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/6614 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1b2d5ba2d7248c6ad201e82a663877e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5474bc735052459c9988afe60c02ede7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1405eb57cce74cd7b65d6782956af62c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6614 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389138f7d5774fbeb38828a9d7a6d163"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19f2c457bc1b4f3bb2673c55a24a6885"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2de9adfa90624337a39a33cb7339f56b"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'text', '__index_level_0__', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 6614\n    })\n    validation: Dataset({\n        features: ['label', 'text', '__index_level_0__', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 350\n    })\n    test: Dataset({\n        features: ['label', 'text', '__index_level_0__', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 818\n    })\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"\ndataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:54:12.655160Z","iopub.execute_input":"2025-08-31T21:54:12.655498Z","iopub.status.idle":"2025-08-31T21:54:12.661967Z","shell.execute_reply.started":"2025-08-31T21:54:12.655453Z","shell.execute_reply":"2025-08-31T21:54:12.661253Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:54:15.360729Z","iopub.execute_input":"2025-08-31T21:54:15.361003Z","iopub.status.idle":"2025-08-31T21:54:15.427091Z","shell.execute_reply.started":"2025-08-31T21:54:15.360981Z","shell.execute_reply":"2025-08-31T21:54:15.426277Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport numpy as np\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(unique_labels)\n\ndef per_label_accuracy(y_true, y_pred, labels):\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    correct_predictions = cm.diagonal()\n    label_totals = cm.sum(axis=1)\n    per_label_acc = np.divide(correct_predictions, label_totals, out=np.zeros_like(correct_predictions, dtype=float), where=label_totals != 0)\n    return dict(zip(labels, per_label_acc))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:54:18.526277Z","iopub.execute_input":"2025-08-31T21:54:18.527020Z","iopub.status.idle":"2025-08-31T21:54:18.533544Z","shell.execute_reply.started":"2025-08-31T21:54:18.526985Z","shell.execute_reply":"2025-08-31T21:54:18.532684Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n\n    decoded_labels = label_encoder.inverse_transform(labels)\n    decoded_preds = label_encoder.inverse_transform(preds)\n\n    precision = precision_score(decoded_labels, decoded_preds, average='weighted')\n    recall = recall_score(decoded_labels, decoded_preds, average='weighted')\n    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n    acc = accuracy_score(decoded_labels, decoded_preds)\n\n    labels_list = list(label_encoder.classes_)\n    per_label_acc = per_label_accuracy(decoded_labels, decoded_preds, labels_list)\n\n    per_label_acc_metrics = {}\n    for label, accuracy in per_label_acc.items():\n        label_key = f\"accuracy_label_{label}\"\n        per_label_acc_metrics[label_key] = accuracy\n\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        **per_label_acc_metrics\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:54:21.033880Z","iopub.execute_input":"2025-08-31T21:54:21.034164Z","iopub.status.idle":"2025-08-31T21:54:21.040844Z","shell.execute_reply.started":"2025-08-31T21:54:21.034143Z","shell.execute_reply":"2025-08-31T21:54:21.040047Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=your_path,\n    num_train_epochs=1,\n    warmup_steps=500,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    logging_steps=10,\n    # evaluation_strategy='steps',\n    eval_steps=100,\n    learning_rate=2e-5,\n    save_steps=1000,\n    gradient_accumulation_steps=2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:54:23.587994Z","iopub.execute_input":"2025-08-31T21:54:23.588653Z","iopub.status.idle":"2025-08-31T21:54:25.374901Z","shell.execute_reply.started":"2025-08-31T21:54:23.588620Z","shell.execute_reply":"2025-08-31T21:54:25.374204Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset_encoded['train'],\n    eval_dataset=dataset_encoded['test'],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T21:54:28.209161Z","iopub.execute_input":"2025-08-31T21:54:28.209489Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1495708527.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"trainer.evaluate()\ntrainer.save_model(your_path)\ntrainer.save_state()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\npipe = pipeline('text-classification', model=your_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_titles = [\n    \"grab an example title\",\n    \"grab another example title\",\n    \"and another xample title\"\n]\n\nfor title in example_titles:\n    result = pipe(title)\n    print(f\"Title: {title}\")\n    print(f\"Output: {result[0]['label']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ref: https://www.kaggle.com/code/neerajmohan/fine-tuning-bert-for-text-classification\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}