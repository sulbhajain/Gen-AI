{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "624c4f60",
      "metadata": {
        "id": "624c4f60"
      },
      "source": [
        "\n",
        "# Contextual Bandits on MovieLens — **PyTorch** (GPU-ready)\n",
        "This notebook implements a GPU-ready pipeline for a **contextual bandit** recommender:\n",
        "\n",
        "1. **Matrix Factorization (Torch)** on a chronological train split → user/item embeddings.  \n",
        "2. MF-based **candidate retrieval** (Top-K per event).  \n",
        "3. A **stochastic logging policy** μ (softmax temperature + ε-mix) to simulate bandit logs with **propensities**.  \n",
        "4. A **reward model** \\( \\hat Q \\) (Torch MLP) predicting click for \\((x,i)\\) pairs.  \n",
        "5. A **policy** \\( \\pi_\\theta \\) (Torch MLP) trained with a **Doubly-Robust (DR)** objective.  \n",
        "6. **Off-Policy Evaluation**: **SNIPS** and **DR** on a held-out bandit log.\n",
        "\n",
        "> **Data expectation**: MovieLens 1M files at `base_dir/ml-1m/` with  \n",
        "> `ratings.dat`, `users.dat`, `movies.dat`. If not present, the notebook will fall back to a synthetic dataset so you can run it end-to-end.\n",
        "\n",
        "> **GPU**: If a CUDA GPU is available, training will use it automatically (`device = 'cuda'`). Otherwise it runs on CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b9273dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b9273dd",
        "outputId": "54bd8ffc-8043-4510-b13d-8467300d5f2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Config ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, math, random, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Repro\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Config\n",
        "base_dir = Path(\"./data\")\n",
        "ml1m_dir = base_dir / \"ml-1m\"    # place ratings.dat, users.dat, movies.dat here\n",
        "\n",
        "# Core params\n",
        "D = 32          # MF embedding dim\n",
        "K = 50          # candidate pool size\n",
        "train_ratio = 0.9\n",
        "rating_thresh = 4\n",
        "\n",
        "# Logging μ\n",
        "tau = 0.7       # softmax temperature\n",
        "eps = 0.05      # epsilon-uniform mix\n",
        "\n",
        "# MF training\n",
        "mf_epochs = 5\n",
        "mf_lr = 0.05\n",
        "mf_reg = 0.0    # using optimizer weight decay instead\n",
        "\n",
        "# Q̂ training\n",
        "qhat_epochs = 5\n",
        "qhat_lr = 1e-3\n",
        "qhat_hid = 128\n",
        "\n",
        "# Policy training\n",
        "policy_epochs = 8\n",
        "policy_lr = 1e-3\n",
        "policy_hid = 128\n",
        "weight_clip = 10.0\n",
        "entropy_coef = 0.01\n",
        "batch_size = 256\n",
        "\n",
        "print(\"Config ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3962249c",
      "metadata": {
        "id": "3962249c"
      },
      "outputs": [],
      "source": [
        "def read_ml1m(ml1m_dir: Path):\n",
        "    rpath = ml1m_dir / \"ratings.dat\"\n",
        "    upath = ml1m_dir / \"users.dat\"\n",
        "    mpath = ml1m_dir / \"movies.dat\"\n",
        "    if not (rpath.exists() and upath.exists() and mpath.exists()):\n",
        "        return None, None, None\n",
        "\n",
        "    ratings = pd.read_csv(rpath, sep=\"::\", engine=\"python\", header=None,\n",
        "                          names=[\"user_id\",\"movie_id\",\"rating\",\"ts\"])\n",
        "    users = pd.read_csv(upath, sep=\"::\", engine=\"python\", header=None,\n",
        "                        names=[\"user_id\",\"gender\",\"age\",\"occupation\",\"zip\"])\n",
        "    movies = pd.read_csv(mpath, sep=\"::\", engine=\"python\", header=None, encoding=\"latin-1\",\n",
        "                         names=[\"movie_id\",\"title\",\"genres\"])\n",
        "    return ratings, users, movies\n",
        "\n",
        "def make_synthetic(n_users=200, n_items=400, n_events=10000):\n",
        "    set_seed(123)\n",
        "    genders = np.random.choice([0,1], size=n_users)  # 0=F,1=M\n",
        "    ages = np.random.choice([1,18,25,35,45,50,56], size=n_users)\n",
        "    occs = np.random.choice(list(range(21)), size=n_users)\n",
        "    users = pd.DataFrame({\n",
        "        \"user_id\": np.arange(1, n_users+1),\n",
        "        \"gender\": genders,\n",
        "        \"age\": ages,\n",
        "        \"occupation\": occs,\n",
        "        \"zip\": [\"00000\"]*n_users\n",
        "    })\n",
        "    genres_all = [\"Action\",\"Comedy\",\"Drama\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"Crime\",\"Animation\",\"Horror\"]\n",
        "    item_genres = [ \"|\".join(np.random.choice(genres_all, size=np.random.randint(1,4), replace=False)) for _ in range(n_items) ]\n",
        "    movies = pd.DataFrame({\n",
        "        \"movie_id\": np.arange(1, n_items+1),\n",
        "        \"title\": [f\"Movie {i}\" for i in range(1, n_items+1)],\n",
        "        \"genres\": item_genres\n",
        "    })\n",
        "    U = np.random.randn(n_users, 8)\n",
        "    V = np.random.randn(n_items, 8)\n",
        "    rows = []\n",
        "    ts = 950_000_000\n",
        "    for _ in range(n_events):\n",
        "        u = np.random.randint(1, n_users+1)\n",
        "        scores = U[u-1] @ V.T + 0.5*np.random.randn(n_items)\n",
        "        i_star = int(np.argmax(scores)) + 1\n",
        "        s = scores[i_star-1]\n",
        "        rating = int(np.clip(np.round(s*0.5+3), 1, 5))\n",
        "        ts += np.random.randint(1, 1000)\n",
        "        rows.append((u, i_star, rating, ts))\n",
        "    ratings = pd.DataFrame(rows, columns=[\"user_id\",\"movie_id\",\"rating\",\"ts\"])\n",
        "    return ratings, users, movies\n",
        "\n",
        "def preprocess_users(users: pd.DataFrame):\n",
        "    # gender\n",
        "    if users[\"gender\"].dtype == object:\n",
        "        g = (users[\"gender\"] == \"M\").astype(int).values.reshape(-1,1)\n",
        "    else:\n",
        "        g = users[\"gender\"].astype(int).values.reshape(-1,1)\n",
        "    age_codes = [1,18,25,35,45,50,56]\n",
        "    age_map = {a:i for i,a in enumerate(age_codes)}\n",
        "    age_idx = users[\"age\"].map(lambda a: age_map.get(int(a), 0)).fillna(0).astype(int).values\n",
        "    age_oh = np.eye(len(age_codes))[age_idx]\n",
        "    occ_max = 21\n",
        "    occ = users[\"occupation\"].astype(int).clip(0, occ_max-1).values\n",
        "    occ_oh = np.eye(occ_max)[occ]\n",
        "    demo = np.concatenate([g, age_oh, occ_oh], axis=1).astype(np.float32)\n",
        "    # index by user_id\n",
        "    demo_df = pd.DataFrame(demo, index=users[\"user_id\"].values)\n",
        "    return demo_df, {\"age_codes\": age_codes, \"occ_max\": occ_max}\n",
        "\n",
        "def preprocess_items(movies: pd.DataFrame):\n",
        "    genre_set = set()\n",
        "    for g in movies[\"genres\"].fillna(\"\").tolist():\n",
        "        for x in str(g).split(\"|\"):\n",
        "            x = x.strip()\n",
        "            if x:\n",
        "                genre_set.add(x)\n",
        "    genres = sorted(list(genre_set))\n",
        "    idx = {g:i for i,g in enumerate(genres)}\n",
        "    G = np.zeros((len(movies), len(genres)), dtype=np.float32)\n",
        "    for r, g in enumerate(movies[\"genres\"].fillna(\"\").tolist()):\n",
        "        for x in str(g).split(\"|\"):\n",
        "            x = x.strip()\n",
        "            if x and x in idx:\n",
        "                G[r, idx[x]] = 1.0\n",
        "    gdf = pd.DataFrame(G, index=movies[\"movie_id\"].values, columns=genres)\n",
        "    return gdf, {\"genres\": genres}\n",
        "\n",
        "def chronological_split(ratings: pd.DataFrame, train_ratio=0.9):\n",
        "    ratings = ratings.sort_values(\"ts\").reset_index(drop=True)\n",
        "    cut = int(len(ratings)*train_ratio)\n",
        "    return ratings.iloc[:cut].copy(), ratings.iloc[cut:].copy()\n",
        "\n",
        "def build_id_maps(ratings: pd.DataFrame):\n",
        "    uids = sorted(ratings[\"user_id\"].unique().tolist())\n",
        "    iids = sorted(ratings[\"movie_id\"].unique().tolist())\n",
        "    umap = {u:i for i,u in enumerate(uids)}\n",
        "    imap = {i:j for j,i in enumerate(iids)}\n",
        "    return umap, imap\n",
        "\n",
        "def make_time_features(ts_series: pd.Series):\n",
        "    dt = pd.to_datetime(ts_series, unit='s', origin='unix')\n",
        "    hod = dt.dt.hour.values\n",
        "    dow = dt.dt.dayofweek.values\n",
        "    hod_sin = np.sin(2*np.pi*hod/24.0)\n",
        "    hod_cos = np.cos(2*np.pi*hod/24.0)\n",
        "    dow_sin = np.sin(2*np.pi*dow/7.0)\n",
        "    dow_cos = np.cos(2*np.pi*dow/7.0)\n",
        "    return np.stack([hod_sin, hod_cos, dow_sin, dow_cos], axis=1).astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5349bb78",
      "metadata": {
        "id": "5349bb78"
      },
      "outputs": [],
      "source": [
        "def train_mf_torch(train_df, umap, imap, D=32, epochs=5, lr=0.05, weight_decay=0.0, thresh=4, batch=4096):\n",
        "    nU, nI = len(umap), len(imap)\n",
        "    U  = nn.Parameter(0.1*torch.randn(nU, D, device=device))\n",
        "    V  = nn.Parameter(0.1*torch.randn(nI, D, device=device))\n",
        "    bu = nn.Parameter(torch.zeros(nU, device=device))\n",
        "    bi = nn.Parameter(torch.zeros(nI, device=device))\n",
        "    g  = nn.Parameter(torch.zeros((), device=device))\n",
        "    opt = torch.optim.SGD([U,V,bu,bi,g], lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    us = torch.tensor(train_df[\"user_id\"].map(umap).values, device=device, dtype=torch.long)\n",
        "    is_ = torch.tensor(train_df[\"movie_id\"].map(imap).values, device=device, dtype=torch.long)\n",
        "    y  = torch.tensor((train_df[\"rating\"].values >= thresh).astype('float32'), device=device)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        idx = torch.randperm(len(us), device=device)\n",
        "        last_loss = 0.0\n",
        "        for b in idx.split(batch):\n",
        "            u, i, r = us[b], is_[b], y[b]\n",
        "            pred = (U[u] * V[i]).sum(-1) + bu[u] + bi[i] + g\n",
        "            loss = F.mse_loss(pred, r)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            last_loss = loss.item()\n",
        "        print(f\"[MF-torch] epoch {ep+1}/{epochs} MSE={last_loss:.4f}\")\n",
        "    with torch.no_grad():\n",
        "        return U.detach(), V.detach(), bu.detach(), bi.detach(), g.detach()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987ad12c",
      "metadata": {
        "id": "987ad12c"
      },
      "outputs": [],
      "source": [
        "def topk_candidates_torch(uvec, V, seen_mask, K):\n",
        "    # uvec: [D], V: [nI, D], seen_mask: [nI] bool\n",
        "    scores = V @ uvec\n",
        "    scores = scores.masked_fill(seen_mask, float('-inf'))\n",
        "    vals, idx = torch.topk(scores, k=K)\n",
        "    return idx, vals\n",
        "\n",
        "def softmax_temp_torch(x, tau):\n",
        "    z = (x / max(tau, 1e-6))\n",
        "    z = z - z.max()\n",
        "    return torch.softmax(z, dim=0)\n",
        "\n",
        "def simulate_bandit_logs_torch(ratings_df, users_df, movies_df, U, V, umap, imap, K=50, tau=0.7, eps=0.05, thresh=4, seed=123):\n",
        "    torch.manual_seed(seed); random.seed(seed); np.random.seed(seed)\n",
        "    # demos & genres\n",
        "    demo_df, _ = preprocess_users(users_df)\n",
        "    genres_df, _ = preprocess_items(movies_df)\n",
        "\n",
        "    max_uid = max(umap.values())+1\n",
        "    max_iid = max(imap.values())+1\n",
        "    demo = torch.zeros((max_uid, demo_df.shape[1]), device=device)\n",
        "    for uid, row in demo_df.iterrows():\n",
        "        if uid in umap:\n",
        "            demo[umap[uid]] = torch.tensor(row.values, device=device, dtype=torch.float32)\n",
        "    genres = torch.zeros((max_iid, genres_df.shape[1]), device=device)\n",
        "    for iid, row in genres_df.iterrows():\n",
        "        if iid in imap:\n",
        "            genres[imap[iid]] = torch.tensor(row.values, device=device, dtype=torch.float32)\n",
        "\n",
        "    # seen sets per user (internal ids)\n",
        "    seen_by_user = [set() for _ in range(max_uid)]\n",
        "    logs = []\n",
        "\n",
        "    ratings_df = ratings_df.sort_values(\"ts\").reset_index(drop=True)\n",
        "    time_feats = torch.tensor(make_time_features(ratings_df[\"ts\"]), device=device)\n",
        "\n",
        "    for idx, row in ratings_df.iterrows():\n",
        "        u_raw = int(row[\"user_id\"]); i_star_raw = int(row[\"movie_id\"])\n",
        "        if u_raw not in umap or i_star_raw not in imap:\n",
        "            continue\n",
        "        u = umap[u_raw]; i_star = imap[i_star_raw]\n",
        "\n",
        "        seen_mask = torch.zeros(V.shape[0], dtype=torch.bool, device=device)\n",
        "        if len(seen_by_user[u])>0:\n",
        "            seen_idx = torch.tensor(list(seen_by_user[u]), device=device, dtype=torch.long)\n",
        "            seen_mask[seen_idx] = True\n",
        "\n",
        "        cand_idx, cand_scores = topk_candidates_torch(U[u], V, seen_mask, K=K)\n",
        "        if (i_star not in cand_idx.tolist()):\n",
        "            # replace worst\n",
        "            minpos = torch.argmin(cand_scores)\n",
        "            cand_idx[minpos] = i_star\n",
        "            cand_scores[minpos] = (U[u] * V[i_star]).sum()\n",
        "\n",
        "        p_soft = softmax_temp_torch(cand_scores, tau)\n",
        "        p_mu = (1.0 - eps) * p_soft + eps * (1.0/len(cand_idx))\n",
        "\n",
        "        a_pos = torch.multinomial(p_mu, 1).item()\n",
        "        a_iid = int(cand_idx[a_pos].item())\n",
        "        logprop = float(p_mu[a_pos].item())\n",
        "        reward = int(a_iid == i_star)\n",
        "\n",
        "        x_t = torch.cat([U[u], demo[u], time_feats[idx]], dim=0).detach()\n",
        "\n",
        "        logs.append({\n",
        "            \"u\": u, \"i_star\": i_star, \"cand_idx\": cand_idx.detach().cpu().numpy().astype('int32'),\n",
        "            \"x_t\": x_t.detach().cpu().numpy().astype('float32'),\n",
        "            \"logprop\": logprop, \"a_pos\": a_pos, \"a_iid\": a_iid, \"reward\": reward\n",
        "        })\n",
        "        seen_by_user[u].add(i_star)\n",
        "\n",
        "    return logs, demo.detach().cpu().numpy(), genres.detach().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fbc99dd",
      "metadata": {
        "id": "9fbc99dd"
      },
      "outputs": [],
      "source": [
        "def make_pair_features(x_t_np, item_iid, U, V, genres_np):\n",
        "    # x_t_np: numpy float32, starts with user embedding of dim D\n",
        "    D = U.shape[1]\n",
        "    u_emb = torch.tensor(x_t_np[:D], device=device)\n",
        "    v_emb = V[item_iid]\n",
        "    uv = u_emb * v_emb\n",
        "    g = torch.tensor(genres_np[item_iid], device=device)\n",
        "    ctx_rest = torch.tensor(x_t_np[D:], device=device)\n",
        "    return torch.cat([u_emb, v_emb, uv, g, ctx_rest], dim=0)\n",
        "\n",
        "class MLPQ(nn.Module):\n",
        "    def __init__(self, in_dim, hid=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid), nn.ReLU(),\n",
        "            nn.Linear(hid, 1)\n",
        "        )\n",
        "    def forward(self, x):  # x: [N, in_dim]\n",
        "        return self.net(x)  # logits\n",
        "\n",
        "def train_qhat_torch(X, y, hid=128, lr=1e-3, epochs=5, batch=4096):\n",
        "    model = MLPQ(X.shape[1], hid).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    crit = nn.BCEWithLogitsLoss()\n",
        "    X_t = torch.tensor(X, device=device, dtype=torch.float32)\n",
        "    y_t = torch.tensor(y, device=device, dtype=torch.float32).view(-1,1)\n",
        "    for ep in range(epochs):\n",
        "        perm = torch.randperm(len(X_t), device=device)\n",
        "        last = 0.0\n",
        "        for b in perm.split(batch):\n",
        "            xb, yb = X_t[b], y_t[b]\n",
        "            logits = model(xb)\n",
        "            loss = crit(logits, yb)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            last = loss.item()\n",
        "        print(f\"[Qhat] epoch {ep+1}/{epochs} BCE={last:.4f}\")\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def qhat_predict_probs(model, X):  # X: [N, in_dim] tensor on device\n",
        "    return torch.sigmoid(model(X)).squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d20f18",
      "metadata": {
        "id": "71d20f18"
      },
      "outputs": [],
      "source": [
        "class PolicyMLP(nn.Module):\n",
        "    def __init__(self, in_dim, hid=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid), nn.ReLU(),\n",
        "            nn.Linear(hid, 1)  # per-candidate logit\n",
        "        )\n",
        "    def forward(self, X):  # X: [B,K,in_dim]\n",
        "        B,K,D = X.shape\n",
        "        z = self.net(X.view(B*K, D)).view(B, K)   # logits per candidate\n",
        "        P = F.softmax(z, dim=1)\n",
        "        return P, z\n",
        "\n",
        "def build_qhat_dataset(logs, U, V, genres_np, neg_per_pos=1, cap=None):\n",
        "    Xs, ys = [], []\n",
        "    it = logs if cap is None else logs[:cap]\n",
        "    for ex in it:\n",
        "        x_t = ex[\"x_t\"]; cand = ex[\"cand_idx\"]; a_pos = ex[\"a_pos\"]; r = ex[\"reward\"]\n",
        "        # positive\n",
        "        Xa = make_pair_features(x_t, int(cand[a_pos]), U, V, genres_np)\n",
        "        Xs.append(Xa.cpu().numpy()); ys.append(r)\n",
        "        # negatives\n",
        "        others = [k for k in range(len(cand)) if k!=a_pos]\n",
        "        random.shuffle(others)\n",
        "        for _ in range(neg_per_pos):\n",
        "            if not others: break\n",
        "            k = others.pop()\n",
        "            Xn = make_pair_features(x_t, int(cand[k]), U, V, genres_np)\n",
        "            Xs.append(Xn.cpu().numpy()); ys.append(0)\n",
        "    X = np.stack(Xs, axis=0).astype(np.float32)\n",
        "    y = np.array(ys, dtype=np.float32)\n",
        "    return X, y\n",
        "\n",
        "def train_policy_dr_torch(logs, U, V, genres_np, qhat_model, K, lr=1e-3, epochs=5, wclip=10.0, ent=0.01, bs=256):\n",
        "    # infer in_dim\n",
        "    x0 = logs[0][\"x_t\"]; i0 = int(logs[0][\"cand_idx\"][0])\n",
        "    phi0 = make_pair_features(x0, i0, U, V, genres_np).to(device)\n",
        "    in_dim = phi0.numel()\n",
        "    policy = PolicyMLP(in_dim, hid=policy_hid).to(device)\n",
        "    opt = torch.optim.AdamW(policy.parameters(), lr=lr)\n",
        "\n",
        "    def make_batch(batch):\n",
        "        B = len(batch)\n",
        "        X_all = []\n",
        "        Q_all = []\n",
        "        a_pos = []\n",
        "        mu = []\n",
        "        r = []\n",
        "        with torch.no_grad():\n",
        "            for ex in batch:\n",
        "                x_t = ex[\"x_t\"]; cand = ex[\"cand_idx\"]; apos = ex[\"a_pos\"]\n",
        "                feats = [make_pair_features(x_t, int(c), U, V, genres_np) for c in cand]\n",
        "                X = torch.stack(feats, 0).to(device)  # [K, in_dim]\n",
        "                Q = qhat_predict_probs(qhat_model, X) # [K]\n",
        "                X_all.append(X); Q_all.append(Q)\n",
        "                a_pos.append(apos); mu.append(ex[\"logprop\"]); r.append(ex[\"reward\"])\n",
        "        X_all = torch.stack(X_all, 0)                 # [B,K,in_dim]\n",
        "        Q_all = torch.stack(Q_all, 0)                 # [B,K]\n",
        "        a_pos = torch.tensor(a_pos, device=device, dtype=torch.long)\n",
        "        mu    = torch.tensor(mu, device=device, dtype=torch.float32)\n",
        "        r     = torch.tensor(r,  device=device, dtype=torch.float32)\n",
        "        return X_all, Q_all, a_pos, mu, r\n",
        "\n",
        "    N = len(logs)\n",
        "    for ep in range(epochs):\n",
        "        random.shuffle(logs)\n",
        "        tot = 0.0; steps = 0\n",
        "        for s in range(0, N, bs):\n",
        "            batch = logs[s:s+bs]\n",
        "            X_all, Q_all, a_pos, mu, r = make_batch(batch)     # [B,K,d], [B,K], [B], [B], [B]\n",
        "            P, logits = policy(X_all)                           # [B,K], [B,K]\n",
        "\n",
        "            direct = (P * Q_all).sum(1)\n",
        "            pi_at = P[torch.arange(P.size(0), device=device), a_pos] + 1e-12\n",
        "            w = (pi_at / (mu + 1e-12)).clamp(max=wclip)\n",
        "            Qa = Q_all[torch.arange(Q_all.size(0), device=device), a_pos]\n",
        "            resid = w * (r - Qa)\n",
        "            H = -(P * torch.log(P + 1e-12)).sum(1)\n",
        "\n",
        "            loss = -(direct + resid + ent*H).mean()\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "\n",
        "            tot += loss.item(); steps += 1\n",
        "        print(f\"[Policy-DR] epoch {ep+1}/{epochs} loss={tot/max(1,steps):.4f}\")\n",
        "    return policy\n",
        "\n",
        "@torch.no_grad()\n",
        "def ope_snips(policy, logs, U, V, genres_np, K):\n",
        "    num, den = 0.0, 0.0\n",
        "    for ex in logs:\n",
        "        x_t = ex[\"x_t\"]; cand = ex[\"cand_idx\"]; a_pos = ex[\"a_pos\"]; mu = ex[\"logprop\"]; r = ex[\"reward\"]\n",
        "        feats = torch.stack([make_pair_features(x_t, int(c), U, V, genres_np) for c in cand], 0).to(device)\n",
        "        P, _ = policy(feats.unsqueeze(0))  # [1,K]\n",
        "        P = P[0]\n",
        "        w = (P[a_pos].item() / (mu + 1e-12))\n",
        "        num += w * r\n",
        "        den += w\n",
        "    return float(num/den) if den > 0 else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def ope_dr(policy, qhat_model, logs, U, V, genres_np, K, wclip=10.0):\n",
        "    vals = []\n",
        "    for ex in logs:\n",
        "        x_t = ex[\"x_t\"]; cand = ex[\"cand_idx\"]; a_pos = ex[\"a_pos\"]; mu = ex[\"logprop\"]; r = ex[\"reward\"]\n",
        "        feats = torch.stack([make_pair_features(x_t, int(c), U, V, genres_np) for c in cand], 0).to(device)\n",
        "        P, _ = policy(feats.unsqueeze(0))  # [1,K]\n",
        "        P = P[0]\n",
        "        Q = qhat_predict_probs(qhat_model, feats)    # [K]\n",
        "        direct = float((P * Q).sum().item())\n",
        "        w = float((P[a_pos].item() / (mu + 1e-12)))\n",
        "        w = min(w, float(wclip))\n",
        "        resid = w * (r - float(Q[a_pos].item()))\n",
        "        vals.append(direct + resid)\n",
        "    return float(np.mean(vals)) if len(vals) else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8c8f52",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e8c8f52",
        "outputId": "19d4ca31-c1b7-49a4-c0d6-01aa88b1a19f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ratings: (1000209, 4) Users: (6040, 5) Movies: (3883, 3)\n",
            "Split: train=900188, test=100021\n",
            "[MF-torch] epoch 1/5 MSE=0.2429\n",
            "[MF-torch] epoch 2/5 MSE=0.2430\n",
            "[MF-torch] epoch 3/5 MSE=0.2413\n",
            "[MF-torch] epoch 4/5 MSE=0.2414\n",
            "[MF-torch] epoch 5/5 MSE=0.2383\n",
            "Logs built — train: 900188, test: 100021\n",
            "Q̂ dataset: (60000, 147) (60000,)\n",
            "[Qhat] epoch 1/5 BCE=0.4797\n",
            "[Qhat] epoch 2/5 BCE=0.2922\n",
            "[Qhat] epoch 3/5 BCE=0.1466\n",
            "[Qhat] epoch 4/5 BCE=0.0848\n",
            "[Qhat] epoch 5/5 BCE=0.0556\n",
            "[Policy-DR] epoch 1/8 loss=-0.1809\n",
            "[Policy-DR] epoch 2/8 loss=-0.2009\n",
            "[Policy-DR] epoch 3/8 loss=-0.2017\n",
            "[Policy-DR] epoch 4/8 loss=-0.2027\n",
            "[Policy-DR] epoch 5/8 loss=-0.2034\n",
            "[Policy-DR] epoch 6/8 loss=-0.2040\n",
            "[Policy-DR] epoch 7/8 loss=-0.2045\n",
            "[Policy-DR] epoch 8/8 loss=-0.2049\n",
            "\\n=== OPE on test log ===\n",
            "SNIPS CTR estimate: 0.7952\n",
            "DR    CTR estimate: 0.1940\n"
          ]
        }
      ],
      "source": [
        "# ==== Driver ====\n",
        "ratings, users, movies = read_ml1m(ml1m_dir)\n",
        "if ratings is None:\n",
        "    print(\"MovieLens 1M files not found; using synthetic dataset for a quick run.\")\n",
        "    ratings, users, movies = make_synthetic(n_users=200, n_items=400, n_events=8000)\n",
        "\n",
        "print(\"Ratings:\", ratings.shape, \"Users:\", users.shape, \"Movies:\", movies.shape)\n",
        "train_df, test_df = chronological_split(ratings, train_ratio=train_ratio)\n",
        "print(f\"Split: train={len(train_df)}, test={len(test_df)}\")\n",
        "\n",
        "umap, imap = build_id_maps(ratings)\n",
        "\n",
        "# Train MF on train\n",
        "U, V, bu, bi, g = train_mf_torch(train_df, umap, imap, D=D, epochs=mf_epochs, lr=mf_lr, weight_decay=mf_reg, thresh=rating_thresh)\n",
        "\n",
        "# Simulate μ logs (different seeds for train/test)\n",
        "train_logs, demo_np, genres_np = simulate_bandit_logs_torch(train_df, users, movies, U, V, umap, imap, K=K, tau=tau, eps=eps, thresh=rating_thresh, seed=123)\n",
        "test_logs,  _,       _         = simulate_bandit_logs_torch(test_df,  users, movies, U, V, umap, imap, K=K, tau=tau, eps=eps, thresh=rating_thresh, seed=456)\n",
        "print(f\"Logs built — train: {len(train_logs)}, test: {len(test_logs)}\")\n",
        "\n",
        "# Build Qhat dataset (positives + negatives from candidate set)\n",
        "Xq, yq = build_qhat_dataset(train_logs, U, V, genres_np, neg_per_pos=2, cap=20000)\n",
        "print(\"Q̂ dataset:\", Xq.shape, yq.shape)\n",
        "\n",
        "# Train Q̂\n",
        "qhat_model = train_qhat_torch(Xq, yq, hid=qhat_hid, lr=qhat_lr, epochs=qhat_epochs, batch=4096)\n",
        "\n",
        "# Train policy with DR\n",
        "policy_model = train_policy_dr_torch(train_logs, U, V, genres_np, qhat_model, K=K,\n",
        "                                     lr=policy_lr, epochs=policy_epochs, wclip=weight_clip, ent=entropy_coef, bs=batch_size)\n",
        "\n",
        "# OPE on held-out test log\n",
        "snips = ope_snips(policy_model, test_logs, U, V, genres_np, K)\n",
        "drval = ope_dr(policy_model, qhat_model, test_logs, U, V, genres_np, K, wclip=weight_clip)\n",
        "print(\"\\\\n=== OPE on test log ===\")\n",
        "print(f\"SNIPS CTR estimate: {snips:.4f}\")\n",
        "print(f\"DR    CTR estimate: {drval:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dqS7NNIGWp8E",
      "metadata": {
        "id": "dqS7NNIGWp8E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}